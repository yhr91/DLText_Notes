\documentclass{article}
\usepackage[utf8]{inputenc}

\usepackage{natbib}
\usepackage{graphicx}
\usepackage{fancyhdr}

\pagestyle{fancy}
\fancyhf{}
\fancyfoot[L]{Unless otherwise specififed, all content is the author's summary of "Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016."}

\title{Deep Learning Textbook - Notes}
\author{yusuf.roohani }
\date{August 2017}

\begin{document}
\maketitle


\section{Probability}

\subsection{Distributions}

\begin{itemize}
    \item Probability mass functions:\\
    State of the random variable $\mapsto$ Probability of the variable taking on that state
    
    \item Joint probability functions:
    Probability distribution over multiple variables
    
    \item 
    For any probability mass function:
    \begin{itemize}
    \item Domain of P should be all possible states of x
    \item $\forall$ $x \in$ x, $ 0 \leq P(x) < 1$
    \item $\sum_{x \in \textrm{x}} P(x) = 1$
    \end{itemize}
\end{itemize}

\section{Neural networks}

\begin{itemize}
    \item \textbf{Softmax} The softmax function is normally used to highlight the largest values and suppress values which are significantly below the maximum value
\end{itemize}

\section{Regularization}
Central problem in machine elarning is to perform well not only on training data, but also on new inputs. \textbf{Regularization refers to any modification to a learning algorithm that is intended to reduce its generalization error but not its training error}

In deep learning, a common approach is to regularize estimators: this is done by trading increased bias for reduced variance. Try to improve effectiveness by making a profitable trade. Models are generally in three regimes
\begin{itemize}
    \item \textbf{Underfitting}: Exclude the true data-generating process
    \item Matched the true data generating process
    \item \textbf{Overfitting}: Include the process but also many other possible generating processes
\end{itemize}
The goal of regularization is to take a model from the third regime to the second regime. What we generally find is that the best fitting model is a large model that has been regularized appropriately.

\subsection{Parameter Norm Penalties}

Limit the capacity of models by adding a paramter norm penalty $\Omega(\theta)$ to the objective function $J$. 
$$ \tilde{J}(\theta; X, y) = J( \theta; X, y) + \alpha\Omega(\theta) $$

where $ \alpha \in [0, \infty) $ is a hyperparamter that weighs the relative contribution of the norm penalty term, $\Omega$, relative to the standard objective function J.

So the training algorithm will decrease both $J$ and some measure of the size of some subset of the parameters $ \theta$.

Generally, for neural networks, we \textbf{only penalize the weights of the affine transformation and not the biases}. Bias controls only a single variable whereas the weights determine how two variables interact. Regularizing bias can induce significant underfitting.

\begin{itemize}
    \item \textbf{$L^2$ Regularization} Also known as the weight decay, this regularization strategy drives the weights closer to the origin by adding a term $ \Omega(\theta) = \frac{1}{2} || w ||^2_2$
    
    So, the objective function can be written as:
    $$ \tilde{J}(w; X, y) = J(w; X, y) + \frac{\alpha}{2}w^Tw $$
    
    And the update step after simplification becomes:
    $$ w \leftarrow (1 - \epsilon\alpha)w - \epsilon \nabla_wJ(w; X,y) $$
    
    Clearly, there is a multiplicative shrinking of the weight vector by a constant factor on each step.
    
    ----
    
    Only directins along which the paramters contribute significantly to reducing the objective function are preserved relatively intact. In others, a small Eigen value of the Hessian tells us that movement in that direction will not significantly increase the gradient.
    
    In the specific case of linear regression, this equation is simplified to:
    $$ w = (X^TX + \alpha I) ^{-1} X^T y $$
    
    Since the diagonal entries of $ (X^TX + \alpha I)$ correspond to the variance of each input feature, we see that $L^2$ regularization causes $X$ to appear as though it has higher variance. The algorithm thus shrinks weights on features whose covariance with the output target is low compared with the added variance .
    
    \item{$L^1$ Regularization} - Here we add the absolute value of the individual parameters to $J$
    
    $$ \Omega(\theta) = ||w||_1 = \sum_i|w_i| $$
    
    
    
\end{itemize}

\section{Extra Notes}

\subsection{Dimensionality reduction}

These are notes made during a talk by Laurens van der Maaten at Google in 2013 \cite{vdMaaten}
\begin{itemize}
    \item \textbf{PCA}
    Linear projection of the data, such that the variance between data points is maximized. Tries to make sure that stuff that is dissimilar ends up far apart, so it's better at preserving large pariwise distances
    
    \item \textbf{Locally linear embedding}
    Attempts to preserve small pairwise distances but tends to cluster all points at the origin
    
    \item \textbf{t-dsitibuted Stochastic Neighbor Embedding (tSNE)}
    A non-linear dimensionality reduction that also preserves small pairwise distances such that similar objects are modeled by similar points and dissimlar objects are modeled by distant points. 
    
    Work with two distributions $p_{i,j}$ in the high-dimensional space and ${q_{i,j}$ in the low-dimensional space, where $i,j$ are data points. For any given point in the high dimensional space, center a gaussian at that point and measure the density of all other points under this Gaussian. Repeat for all points and then renormalize
    
    $$ Missing_equation $$
    
    This gives the similarity between paris of points $i,j$. In practice, conditional probability is computed instead of calculating the joint probability directly. Then, repeat this procedure in low dimensional space, but use a t-distribution for $q_{i,j}$ instead of a Gaussian. 
    
    $$ Missing_equation $$
    
    Now, the goal is to make $q_{i,j}$ represent $p_{i,j}$ as much as possible. So, we use KL divergence to measure the distance between the two probability distributions
    
    \textbf{Why does this work?}
    
    If you have a high value of $p_{i,j}$, then you must make sure that you have a high value of $q_{i,j}$ otherwise you'll have a high penalty, as described by the definition of KL divergence
    
    $$ missing_equation$$
    
    On the other hand, if small $p_{i,j}$ is modeled by large $q_{i,j}$ then the penalty is not as large. Thus, tSNE really emphasizes the local structure of the data.
    
    Also, the reason we use a t disitburion instead of gaussian is because as we go down to lower dimensions, our preference for small scale similarity often causes dissimilar points to be modeled too far apart. The fat tails on a t-distribution allow for that.
    
    
\end{itemize}

\begin{figure}[h!]
\centering
\includegraphics[scale=1.7]{universe.jpg}
\caption{The Universe}
\label{fig:univerise}
\end{figure}

\section{Conclusion}
``I always thought something was fundamentally wrong with the universe'' \citep{adams1995hitchhiker}

\bibliographystyle{plain}
\bibliography{references}

\end{document}
